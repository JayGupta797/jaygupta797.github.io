<!DOCTYPE html>
<html lang="en">
<head>
    <!-- Some meta tags -->
    <title>Jay Gupta - Conformal Prediction in GNNs</title>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta
        name="description"
        content="CS224W Project Blog">

    <!-- Favicon -->
    <link rel="icon" href="../assets/favicon.ico" type="image/x-icon">
    <link rel="icon" href="../assets/favicon.svg" type="image/svg+xml">

    <!-- Custom Styles and Scripts-->
    <link rel="stylesheet" href="../styles/default-styles.css"> 
    <link rel="stylesheet" href="../styles/code-styles.css"> 
    <link rel="stylesheet" href="../styles/table-styles.css"> 
    <script defer src="../scripts/table.js"></script>
    <script defer src="../scripts/theme.js"></script> 

    <!-- Load Prism.js for code blocks -->
    <link id="prism-theme" rel="stylesheet"/>
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/plugins/line-numbers/prism-line-numbers.min.css" rel="stylesheet" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/components/prism-python.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/plugins/line-numbers/prism-line-numbers.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/plugins/normalize-whitespace/prism-normalize-whitespace.min.js"></script>
    
    <!-- Load MathJax for LaTex -->
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Load Google Font -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Fira+Sans:wght@300;400;600&display=swap" rel="stylesheet">
</head>

<body>
    <div class="everything">
    <div class="masthead">
    <div class="masthead__inner-wrap">
        <div class="masthead__menu">
            <nav class="greedy-nav">
                <a class="site-title" href="../index.html">Jay Gupta</a>
                <ul class="visible-links">
                    <li class="masthead__menu-item" id="masthead__menu-photos"><a href="../pages/photos.html">Photos</a></li>
                    <li class="masthead__menu-item" id="masthead__menu-posts"><a href="../pages/posts.html">Posts</a></li>
                    <li class="masthead__menu-item" id="masthead__menu-resume"><a href="../pages/resume.html">Resume</a></li>
                </ul>
                <div class="dark_toggle" onclick="toggleDark()">
                    <svg height=35px width=35px>
                    <circle cx="50%" cy="50%" r="10px" fill="white" id="moon" />
                    <circle cx="50%" cy="50%" r="15px" fill="none" stroke="white" stroke-width="2px" id="ring" />
                    </svg>
                </div>
            </nav>
        </div>
    </div>
    </div>

    <div class="initial-content">
        <div id="main" role="main">
        <div class="sidebar">
            <table id="headerTable">
                <th>
                    <object class="toc" type="image/svg+xml" data="../assets/toc.svg">
                        Your browser does not support SVG
                    </object>
                    On This Page
                </th>
                <tbody id="tableBody">
                </tbody>
            </table>
        </div>

        <div class="mainbar" id="headerContainer">
            <h>Motivation</h>
            <p>
                Graph Neural Networks (GNNs) are powerful machine learning algorithms over graph-structured data. 
                These models have seen success across several domains from Polypharmacy to Recommender Systems.
                However, GNNs lack rigorous uncertainty estimates, limiting their deployment in high-stake settings.
            </p>
            <p>
                Huang et al's 
                <a href="https://arxiv.org/abs/2305.14535">Uncertainty Quantification over Conformalized GNNs</a>
                demonstrates, both theoretically and empirically, the validity of Conformal Prediction in the context of GNNs. 
                We implement conformalized GNNs in the experimental version of the PyG Library as a general-purpose algorithm for classification and regression problems. 
                This addresses the need for uncertainty estimates.
            </p>
            <h>Conformal Prediction</h>
            <p>
                Conformal prediction is a modern framework designed to quantify uncertainty 
                by constructing prediction sets that contain the true outcome with a specified probability.
            </p>
            <p>
                For concreteness, let \( (X_{i}, Y_{i}) \sim P \) be a feature and response pair 
                from a distribution \( P \) over the Cartesian Product \( {X} \times \mathcal{Y} \). 
                Let \( \alpha \in (0, 1) \) denote the miscoverage rate.
                Our task is to find a prediction set \( C \) such that the probability of a observing 
                a new feature and response pair in the band is bounded by the miscoverage rate.
            </p>
            <div class="display-math">
                <p>
                    \[
                        \mathbb{P} \left ( Y_{n + 1} \in \hat{C_{n}}(X_{n + 1}) \ge 1 - \alpha \right )
                    \]
                </p>
            </div>
            <p>
                Remarkably, this is achievable under mild conditions!
                We build intuition in the discussion that follows.
            </p>
            <h2>Toy Example</h2>
            <p>
                Consider an ordered list of integers spanning \( 1 \) through \( n \).
                Now pick a new number and insert it in the list.
                To produce a set of numbers such that the new number is in the set with a specified coverage, we pick the \( \lceil(n + 1)(1 - \alpha) \rceil \) smallest.
            </p>
            <figure>
                <object type="image/svg+xml" data="../assets/conformal-images/fraction.svg" width="100%" height="auto">
                Your browser does not support SVG
                </object>
                <figcaption>Prediction Set in Discrete Setting</figcaption>
            </figure>
            <p>
                At this point, you might think we cheated.
                In particular, we added \( n + 1 \) to the list before constructing our set!
                The insight is that we didn't need to know the new point so long as it was equally likely to be placed anywhere.
                This condition, fancifully called exchangeability, is the only requirement for Conformal Prediction.
                In general, we define the quantile below.
            </p>
            <div class="display-math">
                <p>
                    \[
                        \hat{q} = \lceil (n + 1)(1 - \alpha) \rceil \hspace{0.5em} \text{lowest}
                    \]
                </p>
            </div>
            <p>
                With this quantile on hand, we reason that the prediction set meets coverage with finite sample correction.
            </p>
            <div class="display-math">
                <p>
                    \[
                        \mathbb{P} \left ( 
                            Y_{n + 1} \le \hat{q}
                        \right )
                        \in 
                        \left [ 1 - \alpha, 1 - \alpha + \frac{1}{n + 1} \right ]
                    \]
                </p>
            </div>
            <p>
                While trivial, this example showcases the power of rank (order) statistics.
                If we can introduce an <it>order</it> to our observations, we can construct prediction sets with arbitrary coverage.
            </p>
            <p>
                We extend these ideas to regression.
                Consider a dataset \( D \) partitioned into a disjoint training set \( D_{1} \) and calibration set \( D_{2} \).
                Let \( \hat{f}_{n} \) be a point-predictor trained on \( D_{1} \).
                We compute the residuals.
            </p>
            <div class="display-math">
                <p>
                    \[
                        R_{i} = \lvert \hat{f}_{n}(x_{i} - y_{i}) \rvert 
                        \hspace{0.5em} \text{for} \hspace{0.5em}
                        i \in D_{2}
                    \]
                </p>
            </div>
            <p>
                The residuals are an order statistic.
                As before we compute \( \hat{q}_{n_{2}} \) as the lowest among \( R_{i} \).
                The prediction set \( f_{n_{1}} (x_{i}) \pm \hat{q}_{n_{2}} \) has guaranteed coverage on \( D_{1} \) despite being calibrated on different data.
            </p>
            <p>
                To flesh this out, we will construct a simple example.
                Consider a cubic spline \( f(x) \).
                For a collection of input, output pairs we will add Gaussian Noise drawn from \( \mathcal{N}(\mu, \sigma) \) for \( \mu = 0 \) and \( \sigma = 0.3 \).
                Running the algorithm above, yields the following plot.
            </p>
            <figure>
                <object type="image/svg+xml" data="../assets/conformal-images/bands.svg" width="100%" height="auto">
                Your browser does not support SVG
                </object>
                <figcaption>Conformal Prediction on Cubic Spline</figcaption>
            </figure>
            <p>
                Since our predictor is perfect (by construction) we expect the quantiles to capture the Gaussian Noise.
                In particular, to have coverage \( 1 - \alpha \) we would expect <tt>normalcdf</tt>\( (-\hat{q}, \hat{q}) \approx 1 - \alpha \) in general.
                We tabulate the empirical and expected quantiles below.
            </p>
            <figure>
                <table class="textTable">
                    <tr>
                        <th>Miscoverage</th>
                        <th>Empirical</th>
                        <th>Expected</th>
                    </tr>
                    <tr>
                        <td>\( 0.10 \)</td>
                        <td>\( 0.461 \)</td>
                        <td>\( 0.493 \)</td>
                    </tr>
                    <tr>
                        <td>\( 0.20 \)</td>
                        <td>\( 0.366 \)</td>
                        <td>\( 0.384 \)</td>
                    </tr>
                    <tr>
                        <td>\( 0.30 \)</td>
                        <td>\( 0.291 \)</td>
                        <td>\( 0.311 \)</td>
                    </tr>
                    <tr>
                        <td>\( 0.40 \)</td>
                        <td>\( 0.222 \)</td>
                        <td>\( 0.252 \)</td>
                    </tr>
                    <tr>
                        <td>\( 0.50 \)</td>
                        <td>\( 0.163 \)</td>
                        <td>\( 0.202 \)</td>
                    </tr>
                </table>
                <figcaption>Comparison of Quantiles</figcaption>
            </figure>
            <p>
                The table confirms our expectation.
                It is worth pointing out that as the miscoverage decreases the prediction set grows larger to accommdate.
            </p>
            <h2>General Approach</h2>
            <p>
                A popular tutorial on Conformal Prediction summarizes the steps.
            </p>
            <blockquote>
                <ol>
                    <li>Identify an uncertainty heuristic using the pre-trained model.</li>
                    <li>Define the score function \( s(x, y) \in \mathbb{R} \)</li>
                    <li>Compute \( \hat{q} \) as the \( \frac{\lceil(n + 1)(1 - \alpha) \rceil}{n} \) quantile of calibration scores</li>
                    <li>Form prediction sets \( \mathcal{C}(X) = \{y : s(x, y) \le \hat{q} \} \)</li>
                </ol>
            </blockquote>
            <p>
                While any score function can guarantee coverage, improving it reduces the length of the prediction set.
            </p>
            <div class="display-math">
                <p>
                    \[
                        \text{len} = \int \int_{C} \mathrm{d}\mu(y) \mathrm{d}(P_{X})
                        \hspace{0.5em} \text{and} \hspace{0.5em}
                        \text{cov} = \int \int_{C} \mathrm{d}P_{Y \vert X} \mathrm{d}(P_{X})
                    \]
                </p>
            </div>
            <p>
                If we increase the probability of a response given a feature, namely \( P_{Y \vert X} \),
                then we can meet coverage without inflating the length.
                The argument is due to Tibshirani.
            </p>
            <p>
                In our assessment, the key challenge of conformal prediction is computing a reasonable score function.
                The next sections overview two approaches.
            </p>
            <h2>Prediction Strategy</h2>
            <p>
                A prediction strategy leverages the pre-trained heuristic to score uncertainty.
                The strategy depends on the task at hand.
            </p>
            <!-- <p>
                In a classification setting, you might define \( s(x, y) = 1 - \hat{f}(x)[y] \) where \( \hat{f}(x)[y] \) yields the softmax probability of the correct class.
                After all, a good heuristic approximates the probability of not being in the true class.
                The downside is that we undercover hard examples and overcover easy examples because there is no way to differentiate!
            </p> -->
            <p>
                Adaptive Prediction Sets (APS) is prediction strategy for the classification problems.
                We define the following score function where \( \pi \) is the permutation that ranks the classes from most to least likely.
            </p>
            <div class="display-math">
                <p>
                    \[
                        s(x, y) = \sum_{j = 1}^{k} \hat{f}(x)_{\pi_j(x)} \quad \text{where} \quad y = \pi_k(x)
                    \]
                </p>
            </div>
            <p>
                Intuitively, we want to include top-rank classes until the cumlative sum of their probabilities meets the desired coverage.
                By greedily doing this from most likely to least likely, we ensure the prediction set is minimal.
                For this reason, the prediction sets are feature adaptive.
            </p>
            <p>
                Conformal Quantile Regression (CQR) is a prediction strategy for regression problems.
                We introduce learned functions \( \hat{t}_{\alpha/2}(x) \) and \( \hat{t}_{1 - \alpha/2}(x) \) that estimate the upper and lower bound. 
                They are trained on the following objective known as the Quantile (Pinball) Loss.
            </p>
            <div class="display-math">
                <p>
                    \[
                        L_{\tau}(\hat{t}, y) = 
                        \begin{cases} 
                        (y - \hat{t}) \tau & \text{if } y > \hat{t}, \\
                        (\hat{t} - y)(1 - \tau) & \text{if } y \leq \hat{t}.
                        \end{cases}
                    \]
                </p>
            </div>
            <p>
                With these functions on hand, we construct the score function.
                Intuitively, we want to center our prediction sets on the known label and move up and down such that the desired coverage is met. 
            </p>
            <div class="display-math">
                <p>
                    \[
                        s(x, y) = 
                        \max \left( \hat{t}_{\alpha/2}(x) - y, \; y - \hat{t}_{1 - \alpha/2}(x) \right)
                    \]
                </p>
            </div>
            <p>
                Computing \( \hat{q} \) as before, we arrive at the following prediction band.
            </p>
            <div class="display-math">
                <p>
                    \[
                        \mathcal{C}(x) = 
                        \left[ \hat{t}_{\alpha/2}(x) - \hat{q}, \; \hat{t}_{1 - \alpha/2}(x) + \hat{q} \right].
                    \]
                </p>
            </div>
            </p>
            <h2>Calibration Strategy</h2>
            <p>
                A calibration strategy modifies the score function by adjusting the heuristic itself.
                We introduce four calibration strategies.
            </p>
            <p>
                Temperature Scaling (TS) scales the logits \( z \) by a learned temperature.
                The (negative) log-likelihood is minimized.
            </p>
            <div class="display-math">
                <p>
                    \[
                        \min_T \, -\frac{1}{N} \sum_{i=1}^N \log p_{y_i}^{(T)}
                        \hspace{0.5em} \text{for} \hspace{0.5em}
                        p_{y_i}^{(T)} = \text{softmax} \left( \frac{z_i}{T} \right)_{y_i}
                    \]
                </p>
            </div>
            <p>
                There is clear intuition.
                A temperature greater than \( 1 \) smooths the probabilities, reudicing overconfidence.
                A temperature less than \( 1 \) sharpens the probabilities, reducing underconfidence.
            </p>
            <p>
                There are variants.
                Vector Scaling (VS) generalizes temperature scaling with a temperature for each class and a bias term.
                Ensemble Temperature Scaling (ETS) solves a constrained optimization problem that weights the contributions of different scales.
            </p>
            <p>
                While the methods above are useful, they omit the underlying graph structure.
                Huang et al introduce a Calibration Attention Layer to account for this.
                Taking the original setup and drawing on GATConv along with some tweaks, we compute a node temperature.
            </p>
            <div class="display-math">
                <p>
                    \[
                        T_i = \text{ReLU}(\mathbf{W} \cdot \text{GATConv}_{\theta}(\mathcal{G})_i + b)
                    \]
                </p>
            </div>
            <p>
                By incorporating information from neighboring nodes, with attention weights for each contribution, we can improve performance.
            </p>
            <figure>
                <object type="image/svg+xml" data="../assets/conformal-images/graph.svg" width="100%" height="auto">
                Your browser does not support SVG
                </object>
                <figcaption>Message Passing under Graph Attention Scheme</figcaption>
            </figure>
            <h2>Limitation</h2>
            <p>
                Despite the Ad-Hoc fixes listed above, Conformal Prediction has its limitations.
                For one, Conformal Prediction does not guarantee conditional coverage.
                On average, you would expect the desired coverage.
                However, upon conditioning on a particular class, you might find that the coverage is much lower (or higher) than expected.
            </p>
            <figure>
                <object type="image/svg+xml" data="../assets/conformal-images/conditional.svg" width="100%" height="auto">
                Your browser does not support SVG
                </object>
                <figcaption>Marginal vs Condional Coverage</figcaption>
            </figure>
            <p>
                The figure above, adapted from a popular tutorial, visually summarizes the salient idea.
                This bias may be unacceptable in certain settings.
            </p>
            <p>
                For another, we demand exchangeability.
                That said, Weighted Conformal Prediction is an active and promising direction that accounts for covariate shift.
            </p>
            <h>Implementation</h>
            <p>
                Our implementation of the ideas above was designed with ease-of-use and integration with PyG in mind.
                We introduce an <tt>Uncertain</tt> layer under <tt>Torch.NN</tt> with three files.
                Each file maps directly to a step in the Conformal Prediction pipeline.
            </p>
            <figure>
                <object type="image/svg+xml" data="../assets/conformal-images/directory.svg" width="100%" height="auto">
                Your browser does not support SVG
                </object>
                <figcaption>Directory Structure of Conformal</figcaption>
            </figure>
            <p>
                We took care to include existing PyG functionality where applicable.
                Notably, we use <tt>GATConv</tt> to build the Calibration Attention Layer.
            </p>
            <h2>Interface</h2>
            <p>
                The interface allows users to rapidly apply Conformal Prediction to an existing model.
                Borrowing from <tt>sklearn</tt> we use a fit and predict pattern for familarity and convenience.
            </p>
            <pre class="line-numbers">
                <code class="language-python">
                    from uncertain import Conformal

                    # Create conformal instance
                    conformal = Conformal(
                        model=model,
                        prediction='APS', # APS, RAPS, TPS
                        calibration='TS', # TS, VS, ETS, GATS
                    )

                    # Call Fit and Predict on instance
                    conformal.fit()
                    conformal.predict()
                </code>
            </pre>
            <p>
                The fit step sets up initial state and optionally fits the specified calibrator.
                The predict step runs conformal prediction and returns a <tt>ConformalPrediction</tt> object.
                This object contains the prediction sets along with useful statistics that the user can inspect.
            </p>
            <p>
                There are a couple "gotchas" that should be explicitly mentioned.
                First, the task is inferred from the prediction method.
                Second, the user must specify any train, test, and validation data.
                Handling the split properly is essential for statistically guaranteed coverage as noted above.
            </p>
            <h2>Features</h2>
            <p>
                We provide the user with several ready-made prediction and calibration strategies.
                The full list is shared in the interface.
            </p>
            <p>
                The prediction strategies are generally compact and readable.
                We include simplified snippet that captures the essence of APS.
                This is consistent with the description above.
            </p>
            <pre class="line-numbers">
                <code class="language-python">
                    def aps(smx, labels, n, alpha):
                        # Sort scores descending order
                        pi = smx.argsort(1)[:, ::-1]
                        srt = np.take_along_axis(smx, pi, axis=1).cumsum(axis=1)
                        
                        # Get cumulative probability up to true label
                        scores = np.take_along_axis(srt, pi.argsort(axis=1), axis=1)
                        scores = scores[range(n), labels]
                        
                        # Find threshold for coverage
                        qhat = np.quantile(
                            scores, 
                            np.ceil((n + 1) * (1 - alpha)) / n, 
                            interpolation="higher"
                        )
                </code>
            </pre>
            <p>
                The calibration strategies are standard.
                We include the forward pass of TS that scales the logits by the temperature.
                This is consistent with the description above.
            </p>
            <pre class="line-numbers">
                <code class="language-python">
                    def forward(self, x, edge_index):
                        logits = self.model(x, edge_index)
                        return logits / self.temperature.expand(logits.size())
                </code>
            </pre>
            <p>
                These implementations are housed in the prediction and calibration files respectively.
                The conformal file defines the wrapper which applies the specified methods.
            </p>
            <h>Walkthrough</h>
            <p>
                This is some vanilla text.
                I describe things.
            </p>
            <h>Future Directions</h>
            <p>
                Future PRs should add the following features.
                Functions to directly optimize desirable properties such as efficiency or conditional coverage.
                Weighted Conformal Prediction to hold exchangeability under covariate shift. 
            </p>
            <h>Comment Section</h>
            <div id="utterances-container">
                <script src="https://utteranc.es/client.js"
                        repo="jaygupta797/jaygupta797.github.io"
                        issue-term="pathname"
                        crossorigin="anonymous"
                        >
                </script>
            </div>
        </div>
        </div>
    </div>
    </div>

    <main></main>

    <footer>
        <div class="mastfoot">
            <p>&copy; 2024 Jay Gupta. Powered by vanilla HTML, CSS, and JS.</p>
        </div>
    </footer>

</body>
